{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from IPython.utils.capture import capture_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "from midi_util import read_midi, produce_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path:  /Users/danielrjohnson/Documents/OneDrive/Programming/AIO/Music-Generation/data/\n",
      "# of Files Found:  35\n",
      "(35,)\n",
      "(35,)\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd() + \"/data/\"\n",
    "print(\"Data path: \", path)\n",
    "\n",
    "midi_file_names = [i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "print(\"# of Files Found: \", len(midi_file_names))\n",
    "\n",
    "notes = []\n",
    "durations = []\n",
    "#hide output, gives many long annoying warnings\n",
    "with capture_output():\n",
    "    note_data_by_file = []\n",
    "    duration_data_by_file = []\n",
    "\n",
    "    for fn in midi_file_names:\n",
    "        notes, durations = read_midi(path + fn, allow_chords=False)\n",
    "        note_data_by_file.append(notes)\n",
    "        duration_data_by_file.append(durations)\n",
    "\n",
    "    note_data_by_file = np.array(note_data_by_file)\n",
    "    duration_data_by_file = np.array(duration_data_by_file)\n",
    "\n",
    "#print(note_data_by_file)\n",
    "print(note_data_by_file.shape)\n",
    "#print(duration_data_by_file)\n",
    "print(duration_data_by_file.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Notes Total:  29748\n",
      "# of Unique Notes:  87\n"
     ]
    }
   ],
   "source": [
    "#list comp to flatten\n",
    "flattened_note_data = [element for note_ in note_data_by_file for element in note_]\n",
    "print(\"# of Notes Total: \", len(flattened_note_data))\n",
    "\n",
    "all_unique_notes = list(set(flattened_note_data))\n",
    "print(\"# of Unique Notes: \", len(all_unique_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Notes Occurring At Least 25 Times:  70\n"
     ]
    }
   ],
   "source": [
    "note_counts = dict(Counter(flattened_note_data))\n",
    "\n",
    "MIN_COUNT = 25\n",
    "frequent_notes = [note_ for note_, count in note_counts.items() if count >= MIN_COUNT]\n",
    "print(\"# of Notes Occurring At Least\", MIN_COUNT, \"Times: \", len(frequent_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_note_data = [list(filter(lambda x: x in frequent_notes, midi_data)) for midi_data in note_data_by_file]\n",
    "freq_note_data = np.array(freq_note_data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 32\n",
    "X, y_note, y_duration = [], [], []\n",
    "\n",
    "for notes_of_file, durations_of_file in zip(note_data_by_file, duration_data_by_file): #freq_note_data:\n",
    "    for i in range(0, len(notes_of_file) - n_timesteps):\n",
    "        # (n_timesteps) length note sequence\n",
    "        X.append([notes_of_file[i:i + n_timesteps], durations_of_file[i:i + n_timesteps]])\n",
    "        # the note following that sequence\n",
    "        y_note.append(notes_of_file[i + n_timesteps])\n",
    "        # the duration of the note following that sequence\n",
    "        y_duration.append(durations_of_file[i + n_timesteps])\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y_note = np.array(y_note)\n",
    "y_duration = np.array(y_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A0' 'A1' 'A2' 'A3' 'A4' 'A5' 'A6' 'A7' 'B-0' 'B-1' 'B-2' 'B-3' 'B-4'\n",
      " 'B-5' 'B-6' 'B-7' 'B1' 'B2' 'B3' 'B4' 'B5' 'B6' 'B7' 'C#1' 'C#2' 'C#3'\n",
      " 'C#4' 'C#5' 'C#6' 'C#7' 'C1' 'C2' 'C3' 'C4' 'C5' 'C6' 'C7' 'C8' 'D1' 'D2'\n",
      " 'D3' 'D4' 'D5' 'D6' 'D7' 'E-1' 'E-2' 'E-3' 'E-4' 'E-5' 'E-6' 'E-7' 'E1'\n",
      " 'E2' 'E3' 'E4' 'E5' 'E6' 'E7' 'F#1' 'F#2' 'F#3' 'F#4' 'F#5' 'F#6' 'F#7'\n",
      " 'F1' 'F2' 'F3' 'F4' 'F5' 'F6' 'F7' 'G#1' 'G#2' 'G#3' 'G#4' 'G#5' 'G#6'\n",
      " 'G#7' 'G1' 'G2' 'G3' 'G4' 'G5' 'G6' 'G7']\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined note and duration array shape: (28915, 32, 2)\n"
     ]
    }
   ],
   "source": [
    "unique_x_note = list(set(X[:, 0].ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x_note))\n",
    "x_seq_note = np.array([[x_note_to_int[note] for note in row] for row in X[:, 0]])\n",
    "\n",
    "unique_x_dur = list(set(X[:, 1].ravel()))\n",
    "x_dur_to_int = dict((duration, number) for number, duration in enumerate(unique_x_dur))\n",
    "x_seq_dur = np.array([[x_dur_to_int[duration] for duration in row] for row in X[:, 1]])\n",
    "\n",
    "# ([Nx32x1], [Nx32x1]) -> ([Nx32x2])\n",
    "x_seq_combined = np.array(\n",
    "    [ [ [note, duration] for note, duration in zip(row_note, row_dur) ]\n",
    "        for row_note, row_dur in zip(x_seq_note, x_seq_dur) ]\n",
    ")\n",
    "print(\"combined note and duration array shape:\", x_seq_combined.shape)\n",
    "\n",
    "unique_y_note = list(set(y_note))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y_note)) \n",
    "y_seq_note = np.array([y_note_to_int[note] for note in y_note])\n",
    "\n",
    "unique_y_duration = list(set(y_duration))\n",
    "y_dur_to_int = dict((duration_, number) for number, duration_ in enumerate(unique_y_duration))\n",
    "y_seq_dur = np.array([y_dur_to_int[duration] for duration in y_duration])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_note, x_val_note, y_tr_note, y_val_note = train_test_split(x_seq_combined, y_seq_note, test_size=0.2, random_state=0)\n",
    "x_tr_dur, x_val_dur, y_tr_dur, y_val_dur = train_test_split(x_seq_combined, y_seq_dur, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 2)]           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               67072     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 87)                11223     \n",
      "=================================================================\n",
      "Total params: 94,807\n",
      "Trainable params: 94,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((n_timesteps, 2))\n",
    "x = LSTM(128)(inputs)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "output = Dense(len(unique_x_note), activation=\"softmax\")(x)\n",
    "\n",
    "model_note = tf.keras.Model(inputs, output)\n",
    "\n",
    "model_note.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "model_note.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/yg/0w_l2flj2rg2km6898j0j0_40000gn/T/ipykernel_44655/3950189176.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Epoch 1/50\n",
      "181/181 [==============================] - 5s 23ms/step - loss: 4.1918 - val_loss: 3.9318\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.93182, saving model to models/best_model_note.h5\n",
      "Epoch 2/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.8874 - val_loss: 3.8795\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.93182 to 3.87948, saving model to models/best_model_note.h5\n",
      "Epoch 3/50\n",
      "181/181 [==============================] - 4s 19ms/step - loss: 3.8638 - val_loss: 3.8686\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.87948 to 3.86861, saving model to models/best_model_note.h5\n",
      "Epoch 4/50\n",
      "181/181 [==============================] - 5s 29ms/step - loss: 3.8551 - val_loss: 3.8583\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.86861 to 3.85834, saving model to models/best_model_note.h5\n",
      "Epoch 5/50\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 3.8474 - val_loss: 3.8526\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.85834 to 3.85259, saving model to models/best_model_note.h5\n",
      "Epoch 6/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.8413 - val_loss: 3.8500\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.85259 to 3.85004, saving model to models/best_model_note.h5\n",
      "Epoch 7/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.8359 - val_loss: 3.8447\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.85004 to 3.84474, saving model to models/best_model_note.h5\n",
      "Epoch 8/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.8312 - val_loss: 3.8405\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.84474 to 3.84050, saving model to models/best_model_note.h5\n",
      "Epoch 9/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.8275 - val_loss: 3.8370\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.84050 to 3.83700, saving model to models/best_model_note.h5\n",
      "Epoch 10/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.8231 - val_loss: 3.8331\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.83700 to 3.83310, saving model to models/best_model_note.h5\n",
      "Epoch 11/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.8199 - val_loss: 3.8302\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.83310 to 3.83020, saving model to models/best_model_note.h5\n",
      "Epoch 12/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.8168 - val_loss: 3.8290\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.83020 to 3.82896, saving model to models/best_model_note.h5\n",
      "Epoch 13/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.8132 - val_loss: 3.8300\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3.82896\n",
      "Epoch 14/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.8104 - val_loss: 3.8249\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.82896 to 3.82485, saving model to models/best_model_note.h5\n",
      "Epoch 15/50\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 3.8077 - val_loss: 3.8251\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.82485\n",
      "Epoch 16/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.8055 - val_loss: 3.8237\n",
      "\n",
      "Epoch 00016: val_loss improved from 3.82485 to 3.82372, saving model to models/best_model_note.h5\n",
      "Epoch 17/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.8028 - val_loss: 3.8216\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.82372 to 3.82159, saving model to models/best_model_note.h5\n",
      "Epoch 18/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7998 - val_loss: 3.8200\n",
      "\n",
      "Epoch 00018: val_loss improved from 3.82159 to 3.81998, saving model to models/best_model_note.h5\n",
      "Epoch 19/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7973 - val_loss: 3.8197\n",
      "\n",
      "Epoch 00019: val_loss improved from 3.81998 to 3.81974, saving model to models/best_model_note.h5\n",
      "Epoch 20/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7948 - val_loss: 3.8205\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.81974\n",
      "Epoch 21/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7920 - val_loss: 3.8172\n",
      "\n",
      "Epoch 00021: val_loss improved from 3.81974 to 3.81717, saving model to models/best_model_note.h5\n",
      "Epoch 22/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7902 - val_loss: 3.8166\n",
      "\n",
      "Epoch 00022: val_loss improved from 3.81717 to 3.81658, saving model to models/best_model_note.h5\n",
      "Epoch 23/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7874 - val_loss: 3.8164\n",
      "\n",
      "Epoch 00023: val_loss improved from 3.81658 to 3.81639, saving model to models/best_model_note.h5\n",
      "Epoch 24/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7851 - val_loss: 3.8189\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 3.81639\n",
      "Epoch 25/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7833 - val_loss: 3.8153\n",
      "\n",
      "Epoch 00025: val_loss improved from 3.81639 to 3.81529, saving model to models/best_model_note.h5\n",
      "Epoch 26/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7810 - val_loss: 3.8160\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 3.81529\n",
      "Epoch 27/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7781 - val_loss: 3.8136\n",
      "\n",
      "Epoch 00027: val_loss improved from 3.81529 to 3.81364, saving model to models/best_model_note.h5\n",
      "Epoch 28/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7762 - val_loss: 3.8130\n",
      "\n",
      "Epoch 00028: val_loss improved from 3.81364 to 3.81305, saving model to models/best_model_note.h5\n",
      "Epoch 29/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7736 - val_loss: 3.8141\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 3.81305\n",
      "Epoch 30/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.7715 - val_loss: 3.8111\n",
      "\n",
      "Epoch 00030: val_loss improved from 3.81305 to 3.81114, saving model to models/best_model_note.h5\n",
      "Epoch 31/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.7694 - val_loss: 3.8108\n",
      "\n",
      "Epoch 00031: val_loss improved from 3.81114 to 3.81076, saving model to models/best_model_note.h5\n",
      "Epoch 32/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7675 - val_loss: 3.8124\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 3.81076\n",
      "Epoch 33/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.7650 - val_loss: 3.8114\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 3.81076\n",
      "Epoch 34/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.7631 - val_loss: 3.8105\n",
      "\n",
      "Epoch 00034: val_loss improved from 3.81076 to 3.81046, saving model to models/best_model_note.h5\n",
      "Epoch 35/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.7606 - val_loss: 3.8113\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 3.81046\n",
      "Epoch 36/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.7587 - val_loss: 3.8119\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 3.81046\n",
      "Epoch 37/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.7560 - val_loss: 3.8084\n",
      "\n",
      "Epoch 00037: val_loss improved from 3.81046 to 3.80837, saving model to models/best_model_note.h5\n",
      "Epoch 38/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 3.7534 - val_loss: 3.8105\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 3.80837\n",
      "Epoch 39/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.7509 - val_loss: 3.8111\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 3.80837\n",
      "Epoch 40/50\n",
      "181/181 [==============================] - 4s 25ms/step - loss: 3.7489 - val_loss: 3.8118\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 3.80837\n",
      "Epoch 41/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.7471 - val_loss: 3.8083\n",
      "\n",
      "Epoch 00041: val_loss improved from 3.80837 to 3.80827, saving model to models/best_model_note.h5\n",
      "Epoch 42/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.7439 - val_loss: 3.8077\n",
      "\n",
      "Epoch 00042: val_loss improved from 3.80827 to 3.80771, saving model to models/best_model_note.h5\n",
      "Epoch 43/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.7422 - val_loss: 3.8082\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 3.80771\n",
      "Epoch 44/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.7394 - val_loss: 3.8079\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 3.80771\n",
      "Epoch 45/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.7371 - val_loss: 3.8073\n",
      "\n",
      "Epoch 00045: val_loss improved from 3.80771 to 3.80729, saving model to models/best_model_note.h5\n",
      "Epoch 46/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.7343 - val_loss: 3.8066\n",
      "\n",
      "Epoch 00046: val_loss improved from 3.80729 to 3.80661, saving model to models/best_model_note.h5\n",
      "Epoch 47/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.7312 - val_loss: 3.8060\n",
      "\n",
      "Epoch 00047: val_loss improved from 3.80661 to 3.80596, saving model to models/best_model_note.h5\n",
      "Epoch 48/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.7298 - val_loss: 3.8066\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 3.80596\n",
      "Epoch 49/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 3.7265 - val_loss: 3.8043\n",
      "\n",
      "Epoch 00049: val_loss improved from 3.80596 to 3.80425, saving model to models/best_model_note.h5\n",
      "Epoch 50/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 3.7244 - val_loss: 3.8047\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 3.80425\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcq0lEQVR4nO3de3Bc53nf8e+DXewCu4v7hQAIECBEijeRIiWOxEjO2FLkRL5Uct12ak9UO048Gnc8jjJxxo7dmdhu60w6TlI347QajevGHrn2KLUdK/JFlSzRl1oXkyIl3imKN5AACJC4AyRAAE//2MXiQlAESYDLc/b3mcHs7jkHwPsOpR9fPuc972vujoiIBF9BrhsgIiKLQ4EuIhISCnQRkZBQoIuIhIQCXUQkJKK5+sXV1dXe0tKSq18vIhJIO3fuPOvuNfOdy1mgt7S0sGPHjlz9ehGRQDKzE5c7p5KLiEhIKNBFREJiwYFuZhEz22Vmz8xz7vfN7I3M16/N7PbFbaaIiFzJ1dTQHwMOAKXznDsGvNPde83sPcATwN2L0D4REVmgBY3QzawReB/w9fnOu/uv3b038/FloHFxmiciIgu10JLLV4HPAJMLuPaPgJ/Md8LMHjWzHWa2o7u7e4G/WkREFuKKgW5m7we63H3nAq69j3Sgf3a+8+7+hLtvdfetNTXzTqMUEZFrtJAR+r3AQ2Z2HPgucL+ZPTn3IjPbRLok87C7n1vUVs5wqHOQv372EOeGRpfqV4iIBNIVA93dP+fuje7eAnwIeMHdH5l5jZmtAL4P/Dt3P7wkLc042j3E1148wpkBBbqIyEzX/KSomX0CwN0fB/4CqAL+u5kBjLv71kVp4RzJeLrJI2PjS/HjRUQC66oC3d23A9sz7x+fcfzjwMcXs2GXk4xHABgem7gRv05EJDAC96RoIpb+O2h4VCN0EZGZAhfoqbgCXURkPoEL9EQsXXIZUclFRGSWwAX61E3RIY3QRURmCVygx6MFRApMs1xEROYIXKCbGYlYhOFRlVxERGYKXKADJGNR3RQVEZkjmIEej+imqIjIHAEN9CjDqqGLiMwSyEBP19AV6CIiMwUy0FPxqG6KiojMEchAT8SimrYoIjJHIAM9GY8ypBG6iMgswQz0WEQjdBGROQIZ6Il4lJGxCSYnPddNERG5aQQy0FOZNdFHLqrsIiIyJZCBPrUm+oimLoqIZAUy0Kd2LdKKiyIi04IZ6FMjdD3+LyKSFcxA165FIiKXCGSgT+1apPVcRESmBTLQp/cVVclFRGRKIAM9EZ+qoWuELiIyJZCBnoxNzXLRCF1EZEogA13z0EVELhXIQI9FC4hFChjWtEURkaxABjpAIq5NLkREZgpsoCdj2oZORGSm4AZ6PMKIboqKiGQFNtATGqGLiMwS2EBP7yuqQBcRmRLYQE/EInpSVERkhsAGejKukouIyEwLDnQzi5jZLjN7Zp5za83sJTMbNbM/W9wmzi8Zj2j5XBGRGaJXce1jwAGgdJ5zPcAfAx9YhDYtSDIW1QYXIiIzLGiEbmaNwPuAr8933t273P03wMVFbNvbSsajjI1PcnFi8kb9ShGRm9pCSy5fBT4DXFd6mtmjZrbDzHZ0d3dfz4/KromusouISNoVA93M3g90ufvO6/1l7v6Eu2919601NTXX9bO0a5GIyGwLGaHfCzxkZseB7wL3m9mTS9qqBUhqTXQRkVmuGOju/jl3b3T3FuBDwAvu/siSt+wKptZE11x0EZG0q5nlMouZfQLA3R83szpgB+kZMJNm9ifAencfWJRWzmNqTXSVXERE0q4q0N19O7A98/7xGcc7gcbFbNiVZPcV1U1REREgwE+KJuJTs1w0QhcRgQAHejJTctHDRSIiacEN9KkRum6KiogAAQ707E1RlVxERIAAB3qkwCgqLNAsFxGRjMAGOmQ2udAsFxERIOCBnohFGdEIXUQECHygRxjSTVERESDggZ6KRzUPXUQkI9CBntBG0SIiWYEO9GQsopuiIiIZwQ70uG6KiohMCXagxyJ69F9EJCPQgZ6IRxkZm8Ddc90UEZGcC3Sgp+JRxiedMW0ULSIS7EBPaNciEZGsQAe6NooWEZkW7ECPTW0UrRG6iEigA31q1yLNdBERCXigT+0rqsf/RUQCHui6KSoiMi3QgT5VQ9dNURGRoAe6Si4iIlkBD/RMyUWzXEREgh3oxYURzFRyERGBgAe6mZGMRXVTVESEgAc6pGe6aIQuIhKCQE/GowzrpqiISBgCPaJH/0VECEGgJ2JRPfovIkIIAj0Zi2geuogIYQj0eJQRzXIREQlBoKvkIiICXEWgm1nEzHaZ2TPznDMz+zszO2Jmb5jZHYvbzMtL6KaoiAhwdSP0x4ADlzn3HmB15utR4H9cZ7sWLJWZtqiNokUk3y0o0M2sEXgf8PXLXPIw8C1PexkoN7P6RWrj20rEorjD+YsapYtIflvoCP2rwGeAycucXw60zfh8KnNsFjN71Mx2mNmO7u7uq2nnZWUX6NKNURHJc1cMdDN7P9Dl7jvf7rJ5jl1SA3H3J9x9q7tvrampuYpmXt70vqK6MSoi+W0hI/R7gYfM7DjwXeB+M3tyzjWngKYZnxuB9kVp4RUkta+oiAiwgEB398+5e6O7twAfAl5w90fmXPY08JHMbJdtQL+7dyx+cy81vcmFSi4ikt+i1/qNZvYJAHd/HPgx8F7gCDACfGxRWrcACW1DJyICXGWgu/t2YHvm/eMzjjvwycVs2ELppqiISFoonhQFtISuiOS94Af6VA1dJRcRyXOBD/RETBtFi4hACAI9Hi0gWmC6KSoieS/wgW5m2ldURIQQBDpM7SuqkouI5LfQBLoe/ReRfBeOQI9FGNI8dBHJc6EI9EQsqmmLIpL3QhHoqqGLiIQm0DXLRUQkFIGeiOmmqIhIKAI9FY9ocS4RyXuhCPRELMr5ixNMTGqjaBHJX6EI9KkldFV2EZF8FpJA165FIiLhCPTMmujaV1RE8lk4Aj27JrpG6CKSv8IR6Nk10TVCF5H8FYpAT8S1UbSISCgCPRXXrkUiIqEI9ERMI3QRkVAEelKBLiISjkBPZB8sUslFRPJXKAK9MFJALFqgEbqI5LVQBDqkpy5q2qKI5LPwBHo8qgeLRCSvhSfQY1E9+i8ieS00gZ6IR3RTVETyWmgCPRWPqoYuInktNIGeiGlfURHJb6EJ9GQsqm3oRCSvhSfQ49ooWkTy2xUD3cyKzOxVM3vdzPaZ2ZfmuabCzH5gZm9krr1taZp7eQltFC0ieW4hI/RR4H53vx3YDDxoZtvmXPN5YLe7bwI+Avy3RW3lAiRjUcYmJhkbn7zRv1pE5KZwxUD3tKHMx8LMl8+5bD3ws8z1B4EWM1u2mA29kqldi85r6qKI5KkF1dDNLGJmu4Eu4Dl3f2XOJa8DH8xcexfQDDQuYjuvaGrXoiHV0UUkTy0o0N19wt03kw7pu+apkf8VUJEJ/U8Bu4BLktXMHjWzHWa2o7u7+7oaPlciu6+oAl1E8tNVzXJx9z5gO/DgnOMD7v6xTOh/BKgBjs3z/U+4+1Z331pTU3OtbZ6Xdi0SkXy3kFkuNWZWnnlfDDwAHJxzTbmZxTIfPw78wt0HFrmtb0u7FolIvosu4Jp64JtmFiH9F8BT7v6MmX0CwN0fB9YB3zKzCWA/8EdL1eDLSWmjaBHJc1cMdHd/A9gyz/HHZ7x/CVi9uE27OonMTdG+8xdz2QwRkZwJzZOiyyuKqS8r4hu/OsbFCc1FF5H8E5pAj0cjfPGhDRzsHOQbv7rkfqyISOiFJtABfm9DHQ+sW8Z/ff4wbT0juW6OiMgNFapAB/jSwxsoMOMvfrgX97kPtIqIhFfoAn15eTF/+u5befFQNz/Z25nr5oiI3DChC3SAP7inhfX1pXzx6X0MXNCsFxHJD6EM9GikgL/84Ea6h0b5m2cP5bo5IiI3RCgDHWBzUzkf2dbMt14+wettfblujojIkgttoAN8+vfWUJOK87nv72Fcc9NFJORCHeilRYV88aEN7O8Y4AtP79OyACISaqEOdID33FbHH9zTwrdfOcnv/M3Pefr1dk1nFJFQCn2gmxlffGgD3/v391BdEuOPv7OLDz3xMgc7b+hikCIiSy70gT7lzuYKfvjJd/Dlf3kbh84M8r6/+xVffHof/VrMS0RCIm8CHSBSYPz+3c28+Ol38eG7mvjWS8d551de5IlfvMWFi9oYQ0SCLa8CfUpFMsZ//sBG/vlT72BTYzl/+eODvPMrL/LtV05opUYRCay8DPQpGxrK+NYf3sV3H91GY0WC//CDvbz7b3/OD3efZnJSN05FJFgsVzM+tm7d6jt27MjJ756Pu/PCwS6+8uwhDnYOsmZZCR+9p4WHNzeQjC9kYycRkaVnZjvdfeu85xTos01OOv/8RjuP//woBzoGKIlH+eAdy3lkWzOrl5XkunkikucU6NfA3XntZB9PvnyCH73RwdjEJNtaK3lkWzPvXr+MeDSS6yaKSB5SoF+nc0OjPLXjFE++fILTfecpTxTyLzY18ME7lrO5qRwzy3UTRSRPKNAXycSk88s3u/n+a6d5dl8no+OTtNYk+Vd3NPKBLctZXl6c6yaKSMgp0JfAwIWL/GRPB9977TSvHuvBDO5YUcGDG+p48LY6mioTuW6iiISQAn2JtfWM8E+7TvPTfZ3sa08vKbChoTQb7qtqUyrLiMiiUKDfQCfPjfDsvk5+uq+TnSd6gfS2eNtaq7i7tZLfaq2isaJYAS8i10SBniNdAxd4dv8Zfn3kLK8c66FneAyAhrIi7m6t4p5bqrhvbS3VqXiOWyoiQaFAvwm4O292DfHK0XO8fLSHV46d4+zQGGawpamcB9Yv44F1y1it8oyIvA0F+k3I3dnfMcDz+7t4/sAZ9pzuB2BFZYL719ZyZ3MFW1aUs7xc5RkRmaZAD4DO/gv87OAZnt9/hpeOnuPCxfQiYTUlcbY0lbNlRQWbm8rZ1FimpQhE8pgCPWAuTkxyqHOQXSd72XWyj9dO9nL83AgABQara0vY1FjG7U3lbG4qZ01dCYWRvF5nTSRvKNBDoGd4jN1tvbze1s8bp/p4/VR/9iZrLFrA5sZy7m6t5O6VVdzRXE4iplG8SBgp0EPI3TnVe57XT/Wx+2Qfvznew972ASYmnWiBsbGxjLtXVnFncwXrG0ppKCtSLV4kBBToeWJodJwdx3t45VgPrx7r4Y1TfVycSP/5licKWV9fyoaGUtY3lLK+vozWmqRKNSIB83aBrn+Xh0gqHuVda2p515paAM6PTbC/Y4D97f3s7xhgX/sA33zpBGPj6RuusUgBq2pTrKsvZV19CevqS1lTV0JVMqbRvEgAKdBDrDgW4c7mCu5srsgeG5+Y5K3uYfZ39HOwY5ADnYP84s1uvvfaqew15YlCWquT3FKT4pbaVPp9bYqWqiSRAgW9yM3qioFuZkXAL4B45vr/4+5fmHNNGfAksCJzzV+7+/9a/ObK9YpGClhTV8KauhLYMn387NAoBzsGOdg5wNGzw7zVNcT2w938487poI9H09+7vr40M6ovZW19CaVFhTnoiYjMdcUauqX/7Z109yEzKwR+BTzm7i/PuObzQJm7f9bMaoBDQJ27j13u56qGHgz95y9ytHuII11DHOwc5EDHAAc6BugduZi9prkqwW3Ly9i0vIyNy8vYsLyMsmKFvMhSuK4auqcTfyjzsTDzNfdvAQdKMuGfAnqA8Wtusdw0yooL2bKigi0rpss27s6ZgVEOdAywv2OAvaf7eb2tjx+90ZG9pqUqwYaGMm5dVsKty1LcWldCc2WCqG7CiiyZBdXQzSwC7ARWAX/v7q/MueRrwNNAO1AC/Ft3n5zn5zwKPAqwYsWK62i25JKZUVdWRF1ZEfetrc0e7xkeY+/pfvac7mfPqX72tffz470dTP0jMBYtYFVNKhvwt9aWcOuyEhoriilQbV7kul3VtEUzKwd+AHzK3ffOOP6vgXuBPwVuAZ4Dbnf3gcv9LJVc8sP5sQmOdA1x6MwghzNfhzoH6ei/kL2muDDCqtoUty4rYUNDKbctL2N9QykpLXEgcolFm7bo7n1mth14ENg749THgL/KlGeOmNkxYC3w6rU1WcKiOBZhY2MZGxvLZh0fuHCRN88M8eaZQQ6fGeLNrkF+fnj2bJvW6iTrG0rZ0FDGyuoEDeXFLC8vplLTKkXmtZBZLjXAxUyYFwMPAP9lzmUngd8Bfmlmy4A1wNHFbqyER2lR4SVTKiG9hvy+9nRdfm97P7tO9vHMjNo8pGfbLC8vZnlFMbfUpLKj+lW1KT0oJXltISP0euCbmTp6AfCUuz9jZp8AcPfHgf8E/IOZ7QEM+Ky7n12qRkt41ZYWUVs6uzbfP3KRtt4RTvedpz3zdbrvPKd6z/PUjjZGxiaAdI1+bV0JGxrKWFdfwi01KVprktSVatkDyQ969F8CbWLSOXZ2mH3t/dMj+9P9DFyYnmSViEVorUnSWp1iZXWSpsoEjRXFNFUmqCst0sNSEihay0XyytS0yqPdQ7yVeUjq6NlhjnYPcbrvPDP/k48WGPXlRTRVJGipTmafkG2tSdJYkVDYy01Ha7lIXpk5rfKeVdWzzo2OT9DRd4FTvedp6x3hVO8Ip3rPc7JnhB/v6aBvxgNTsUgBzVUJ1tWXsqmxjNuWl7GhoZQSPRkrNykFuuSVeDRCS3WSlurkvOd7hsc42j3E0e5h3jo7xFtdw+w43sPTr7dnr2mtTrKxsYzVtalM+SZBU2UxNam4avWSUwp0kRkqkzEqk5Vsbamcdfzs0Gj2gak9p/t55WgPP9zdPuuaosICmioSNFclWVOXnle/tq6UldVJYlHNvpGlp0AXWYDqVJz71tRy35rp2TcjY+Pp0k3PSPa1rXeEo93DbD/UxfhkulgfLTBaa5KsXlbCyqr0vw5WVidoqUpqTr0sKgW6yDVKxKKZtWpKLjk3Nj7J0bNDHOqcfjp27+l+frq3k4nJ6buyJUVRWqZCviqRLQe1VicpT8RuZHckBBToIksgPSe+lLV1pbOOj41Pcqp3hOPnhjl+Nv167Owwu9t6+dEb7czIesoThayYmmJZkX5trEzQlJlyGY9GbnCv5GanQBe5gWLRAlprUrTWpC45Nzo+QVvPeY6dHeb42WGOnRumrWeEgx2DPL+/i7GJ6fXuIgVGS1WCtXXpXabS9foSmio11TKfKdBFbhLxaHqRslW1l4b95KTTPTQ6q05/sHOQvXNXtIwUsLyiOD2aryimsWL6IarVtSlNuQw5BbpIABQUGMtKi1hWWnTJDJyRsXEOnxnicOcgb3UPcSqzLMJz+89wdmj2HjPLy4tZm9mxak1dehZOc1WCokKVb8JAgS4ScIlYlM1N5WxuKr/k3MjYOO195zl2doTDZwY52DnIoc4Bfn64OzsLB9KzeJoqp2v1TZUJVtWmWFtXolF9gOjRf5E8NDo+wdHuYQ6fGeTkuZHsk7NtvSO0912YNRNnRWUiu4/s+oZS1talNyXRdMvc0KP/IjJLPBrJbvQ91/jEJB39Fzh8ZjC7zeCBjkGe3d+ZrdWn4lFWL0uP4G9dlinhLCuhKhW/wT2RmTRCF5EFGR4dz5Rs0mWbQ5n59TM3DK9OxVlTl2LNstL0a10pq2tTJLX71KLRCF1ErlsyHr1kUxL39Oybw51DHOwcyD5E9Z1XT3L+4kT2upqSOM2VCVZUJWiuTLKiqpgVlUmaKoqpTsW1p+wiUaCLyDUzM2pLiqgtKeIdq6dXtpycdNp6RzjUOcibXUOcODfMiXMjvPTWOb7/2ulZP6MwYtSXpbcXTG8zWMTqZSVsbipXrf4qKdBFZNEVFBjNVUmaq5L87obZ5y5cnOBU7wgnzo1kdp+6kN2N6v8dOcuZwQvZWn11Ks7mpnK2rChnS1M56+pLKU8UKuQvQ4EuIjdUUWGEVbUlrKq9dA0cSC+PcKhzkF1tvew+2cfutj6eP3Amez4Ri9AwYzTfUJaeZrm+oZTW6iTRPN5XVoEuIjeVWLSAjY1lbGws4yO/lT7WNzLG7rY+jnQN0d53Ib23bP959rf3z3p4Kh4tYF19aXbj8A0NpayqTZGI5UfUaZaLiATahYsTnDg3Mmtf2f0dAwzO2Fe2oayIW2pT3FKTyrwmWVtXSmUyeCtaapaLiIRWUWEku5TBB+9IH3N32nrOs7+jnyNdQ7zVPcxb3UP84442hsemZ9/UlxWxPvPA1NRrU0UisLNuFOgiEjpmxoqq9DTJmdydzoELHOka4mDHIPs7BtjfPsD2w93Zp2NLi6Lc3lTOlhUVbGkq5/am8sCM5BXoIpI3zNJTJOvLivnt1TXZ4xcuTnD4zCD72gd441Q/u0728rUX3syuT99clWBdXSk1JXFqSuJUp6ZeYzSUF1NbcnPsJ6tAF5G8V1QYYVNjOZsay/nwXeljw6Pj7Dndz+62Pnad7OVI9xAvHztH34wnY6fUlRZxZ3MFd2QevFpfX5qTfWQV6CIi80jGo2xrrWJba9Ws46PjE5wbGuPs0Cjdg6Oc7Blh18k+dp7o5Ud7OoD0bJtNjWWsqi1hZXV64/CV1UlWVC7tUsUKdBGRqxCPTs+Dn/Kxe9Ovnf0XeO1kLztP9LLrZC8/3dsxa60bM6gvLeIP37GSj/9266K3TYEuIrJI6sqKeO/Get67sT57rH/kYnoP2cw+sifODVNTsjSrUirQRUSWUFmikNsT6dkySy1/n5EVEQkZBbqISEgo0EVEQkKBLiISEgp0EZGQUKCLiISEAl1EJCQU6CIiIZGzDS7MrBs4cY3fXg2cXcTmBEm+9l39zi/q9+U1u3vNfCdyFujXw8x2XG7HjrDL176r3/lF/b42KrmIiISEAl1EJCSCGuhP5LoBOZSvfVe/84v6fQ0CWUMXEZFLBXWELiIicyjQRURCInCBbmYPmtkhMztiZn+e6/YsFTP7hpl1mdneGccqzew5M3sz81qRyzYuBTNrMrMXzeyAme0zs8cyx0PddzMrMrNXzez1TL+/lDke6n5PMbOIme0ys2cyn0PfbzM7bmZ7zGy3me3IHLuufgcq0M0sAvw98B5gPfBhM1uf21YtmX8AHpxz7M+Bn7n7auBnmc9hMw582t3XAduAT2b+jMPe91Hgfne/HdgMPGhm2wh/v6c8BhyY8Tlf+n2fu2+eMff8uvodqEAH7gKOuPtRdx8Dvgs8nOM2LQl3/wXQM+fww8A3M++/CXzgRrbpRnD3Dnd/LfN+kPT/5MsJed89bSjzsTDz5YS83wBm1gi8D/j6jMOh7/dlXFe/gxboy4G2GZ9PZY7li2Xu3gHp4ANqc9yeJWVmLcAW4BXyoO+ZssNuoAt4zt3zot/AV4HPAJMzjuVDvx34v2a208wezRy7rn4HbZNom+eY5l2GkJmlgO8Bf+LuA2bz/dGHi7tPAJvNrBz4gZndluMmLTkzez/Q5e47zexdOW7OjXavu7ebWS3wnJkdvN4fGLQR+imgacbnRqA9R23JhTNmVg+Qee3KcXuWhJkVkg7zb7v79zOH86LvAO7eB2wnfQ8l7P2+F3jIzI6TLqHeb2ZPEv5+4+7tmdcu4AekS8rX1e+gBfpvgNVmttLMYsCHgKdz3KYb6Wngo5n3HwV+mMO2LAlLD8X/J3DA3f92xqlQ993MajIjc8ysGHgAOEjI++3un3P3RndvIf3/8wvu/ggh77eZJc2sZOo98LvAXq6z34F7UtTM3ku65hYBvuHuX85ti5aGmX0HeBfp5TTPAF8A/gl4ClgBnAT+jbvPvXEaaGb2DuCXwB6ma6qfJ11HD23fzWwT6ZtgEdIDrafc/T+aWRUh7vdMmZLLn7n7+8PebzNrJT0qh3Tp+3+7+5evt9+BC3QREZlf0EouIiJyGQp0EZGQUKCLiISEAl1EJCQU6CIiIaFAFxEJCQW6iEhI/H9wdt9cFLO9UwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mc = ModelCheckpoint('models/best_model_note.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "device = tf.device('/device:GPU:0') if tf.config.list_physical_devices(\"GPU\") else tf.device('/device:CPU:0')\n",
    "\n",
    "with device:\n",
    "    history = model_note.fit(x=x_tr_note, y=y_tr_note,\n",
    "                        batch_size=128, epochs=50, \n",
    "                        validation_data=(x_val_note, y_val_note),\n",
    "                        verbose=1, callbacks=[mc]\n",
    "    )\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 2)]           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               67072     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 84,874\n",
      "Trainable params: 84,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((n_timesteps, 2))\n",
    "x = LSTM(128)(inputs)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "output = Dense(len(unique_x_dur), activation=\"softmax\")(x)\n",
    "\n",
    "model_dur = tf.keras.Model(inputs, output)\n",
    "\n",
    "model_dur.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "model_dur.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "181/181 [==============================] - 5s 23ms/step - loss: 1.7959 - val_loss: 1.6600\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.65996, saving model to models/best_model_dur.h5\n",
      "Epoch 2/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6462 - val_loss: 1.6417\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.65996 to 1.64166, saving model to models/best_model_dur.h5\n",
      "Epoch 3/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 1.6360 - val_loss: 1.6370\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.64166 to 1.63704, saving model to models/best_model_dur.h5\n",
      "Epoch 4/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 1.6329 - val_loss: 1.6362\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.63704 to 1.63622, saving model to models/best_model_dur.h5\n",
      "Epoch 5/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 1.6314 - val_loss: 1.6352\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.63622 to 1.63523, saving model to models/best_model_dur.h5\n",
      "Epoch 6/50\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 1.6303 - val_loss: 1.6350\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.63523 to 1.63499, saving model to models/best_model_dur.h5\n",
      "Epoch 7/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 1.6297 - val_loss: 1.6388\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.63499\n",
      "Epoch 8/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6293 - val_loss: 1.6356\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.63499\n",
      "Epoch 9/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6288 - val_loss: 1.6373\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.63499\n",
      "Epoch 10/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6279 - val_loss: 1.6333\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.63499 to 1.63329, saving model to models/best_model_dur.h5\n",
      "Epoch 11/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6276 - val_loss: 1.6336\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.63329\n",
      "Epoch 12/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6263 - val_loss: 1.6344\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.63329\n",
      "Epoch 13/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6254 - val_loss: 1.6321\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.63329 to 1.63214, saving model to models/best_model_dur.h5\n",
      "Epoch 14/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6239 - val_loss: 1.6302\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.63214 to 1.63016, saving model to models/best_model_dur.h5\n",
      "Epoch 15/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6228 - val_loss: 1.6287\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.63016 to 1.62873, saving model to models/best_model_dur.h5\n",
      "Epoch 16/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6215 - val_loss: 1.6282\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.62873 to 1.62824, saving model to models/best_model_dur.h5\n",
      "Epoch 17/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6196 - val_loss: 1.6258\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.62824 to 1.62579, saving model to models/best_model_dur.h5\n",
      "Epoch 18/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6176 - val_loss: 1.6246\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.62579 to 1.62457, saving model to models/best_model_dur.h5\n",
      "Epoch 19/50\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6168 - val_loss: 1.6249\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.62457\n",
      "Epoch 20/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 1.6146 - val_loss: 1.6264\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.62457\n",
      "Epoch 21/50\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 1.6115 - val_loss: 1.6214\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.62457 to 1.62137, saving model to models/best_model_dur.h5\n",
      "Epoch 22/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 1.6098 - val_loss: 1.6196\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.62137 to 1.61962, saving model to models/best_model_dur.h5\n",
      "Epoch 23/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 1.6073 - val_loss: 1.6178\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.61962 to 1.61780, saving model to models/best_model_dur.h5\n",
      "Epoch 24/50\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 1.6047 - val_loss: 1.6164\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.61780 to 1.61640, saving model to models/best_model_dur.h5\n",
      "Epoch 25/50\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 1.6032 - val_loss: 1.6188\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.61640\n",
      "Epoch 26/50\n",
      "179/181 [============================>.] - ETA: 0s - loss: 1.6018"
     ]
    }
   ],
   "source": [
    "mc = ModelCheckpoint('models/best_model_dur.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "device = tf.device('/device:GPU:0') if tf.config.list_physical_devices(\"GPU\") else tf.device('/device:CPU:0')\n",
    "\n",
    "with device:\n",
    "    history = model_dur.fit(x=x_tr_dur, y=y_tr_dur,\n",
    "                        batch_size=128, epochs=50, \n",
    "                        validation_data=(x_val_dur, y_val_dur),\n",
    "                        verbose=1, callbacks=[mc]\n",
    "    )\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_note_sequence = x_val_note[np.random.randint(0, len(x_val_note))]\n",
    "# initial_duration_sequence = x_val_dur[np.random.randint(0, len(x_val_dur))]\n",
    "initial_sequence = x_val_note[np.random.randint(0, len(x_val_note))]\n",
    "x_int_to_note = {v: k for k, v in x_note_to_int.items()}\n",
    "x_int_to_dur = {v: k for k, v in x_dur_to_int.items()}\n",
    "\n",
    "produce_song(initial_sequence, x_int_to_note, x_int_to_dur, \n",
    "            n_notes=50, midi_file_path=\"songs/song.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
