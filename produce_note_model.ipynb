{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from IPython.utils.capture import capture_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "from midi_util import read_midi, produce_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path:  /Users/danielrjohnson/Documents/OneDrive/Programming/AIO/Music-Generation/data/\n",
      "# of Files Found:  35\n",
      "(35,)\n",
      "(35,)\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd() + \"/data/\"\n",
    "print(\"Data path: \", path)\n",
    "\n",
    "midi_file_names = [i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "print(\"# of Files Found: \", len(midi_file_names))\n",
    "\n",
    "notes = []\n",
    "durations = []\n",
    "#hide output, gives many long annoying warnings\n",
    "with capture_output():\n",
    "    note_data_by_file = []\n",
    "    duration_data_by_file = []\n",
    "\n",
    "    for fn in midi_file_names:\n",
    "        notes, durations = read_midi(path + fn)\n",
    "        note_data_by_file.append(notes)\n",
    "        duration_data_by_file.append(durations)\n",
    "\n",
    "    note_data_by_file = np.array(note_data_by_file)\n",
    "    duration_data_by_file = np.array(duration_data_by_file)\n",
    "\n",
    "#print(note_data_by_file)\n",
    "print(note_data_by_file.shape)\n",
    "#print(duration_data_by_file)\n",
    "print(duration_data_by_file.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Notes Total:  29748\n",
      "# of Unique Notes:  993\n"
     ]
    }
   ],
   "source": [
    "#list comp to flatten\n",
    "flattened_note_data = [element for note_ in note_data_by_file for element in note_]\n",
    "print(\"# of Notes Total: \", len(flattened_note_data))\n",
    "\n",
    "all_unique_notes = list(set(flattened_note_data))\n",
    "print(\"# of Unique Notes: \", len(all_unique_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Notes Occurring At Least 25 Times:  216\n"
     ]
    }
   ],
   "source": [
    "note_counts = dict(Counter(flattened_note_data))\n",
    "\n",
    "MIN_COUNT = 25\n",
    "frequent_notes = [note_ for note_, count in note_counts.items() if count >= MIN_COUNT]\n",
    "print(\"# of Notes Occurring At Least\", MIN_COUNT, \"Times: \", len(frequent_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_note_data = [list(filter(lambda x: x in frequent_notes, midi_data)) for midi_data in note_data_by_file]\n",
    "freq_note_data = np.array(freq_note_data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 32\n",
    "X, y_note, y_duration = [], [], []\n",
    "\n",
    "for notes_of_file, durations_of_file in zip(note_data_by_file, duration_data_by_file): #freq_note_data:\n",
    "    for i in range(0, len(notes_of_file) - n_timesteps):\n",
    "        # (n_timesteps) length note sequence\n",
    "        X.append([notes_of_file[i:i + n_timesteps], durations_of_file[i:i + n_timesteps]])\n",
    "        # the note following that sequence\n",
    "        y_note.append(notes_of_file[i + n_timesteps])\n",
    "        # the duration of the note following that sequence\n",
    "        y_duration.append(durations_of_file[i + n_timesteps])\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y_note = np.array(y_note)\n",
    "y_duration = np.array(y_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined note and duration array shape: (28915, 32, 2)\n"
     ]
    }
   ],
   "source": [
    "unique_x_note = list(set(X[:, 0].ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x_note))\n",
    "x_seq_note = np.array([[x_note_to_int[note] for note in row] for row in X[:, 0]])\n",
    "\n",
    "unique_x_dur = list(set(X[:, 1].ravel()))\n",
    "x_dur_to_int = dict((duration, number) for number, duration in enumerate(unique_x_dur))\n",
    "x_seq_dur = np.array([[x_dur_to_int[duration] for duration in row] for row in X[:, 1]])\n",
    "\n",
    "# ([Nx32x1], [Nx32x1]) -> ([Nx32x2])\n",
    "x_seq_combined = np.array(\n",
    "    [ [ [note, duration] for note, duration in zip(row_note, row_dur) ]\n",
    "        for row_note, row_dur in zip(x_seq_note, x_seq_dur) ]\n",
    ")\n",
    "print(\"combined note and duration array shape:\", x_seq_combined.shape)\n",
    "\n",
    "unique_y_note = list(set(y_note))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y_note)) \n",
    "y_seq_note = np.array([y_note_to_int[note] for note in y_note])\n",
    "\n",
    "unique_y_duration = list(set(y_duration))\n",
    "y_dur_to_int = dict((duration_, number) for number, duration_ in enumerate(unique_y_duration))\n",
    "y_seq_dur = np.array([y_dur_to_int[duration] for duration in y_duration])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_note, x_val_note, y_tr_note, y_val_note = train_test_split(x_seq_combined, y_seq_note, test_size=0.2, random_state=0)\n",
    "x_tr_dur, x_val_dur, y_tr_dur, y_val_dur = train_test_split(x_seq_combined, y_seq_dur, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 2)]           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               67072     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "=================================================================\n",
      "Total params: 100,096\n",
      "Trainable params: 100,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((n_timesteps, 2))\n",
    "x = LSTM(128)(inputs)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "output = Dense(128, activation=\"softmax\")(x)\n",
    "\n",
    "model_note = tf.keras.Model(inputs, output)\n",
    "\n",
    "model_note.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "model_note.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "181/181 [==============================] - 5s 22ms/step - loss: 0.5762 - val_loss: 0.5128\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51282, saving model to models/best_model_note.h5\n",
      "Epoch 2/10\n",
      "181/181 [==============================] - 3s 19ms/step - loss: 0.5101 - val_loss: 0.4843\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51282 to 0.48428, saving model to models/best_model_note.h5\n",
      "Epoch 3/10\n",
      "181/181 [==============================] - 3s 19ms/step - loss: 0.4916 - val_loss: 0.4749\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48428 to 0.47491, saving model to models/best_model_note.h5\n",
      "Epoch 4/10\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 0.4859 - val_loss: 0.4703\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.47491 to 0.47032, saving model to models/best_model_note.h5\n",
      "Epoch 5/10\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 0.4832 - val_loss: 0.4685\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.47032 to 0.46853, saving model to models/best_model_note.h5\n",
      "Epoch 6/10\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 0.4817 - val_loss: 0.4642\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.46853 to 0.46419, saving model to models/best_model_note.h5\n",
      "Epoch 7/10\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 0.4801 - val_loss: 0.4649\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.46419\n",
      "Epoch 8/10\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 0.4795 - val_loss: 0.4672\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.46419\n",
      "Epoch 9/10\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 0.4791 - val_loss: 0.4647\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.46419\n",
      "Epoch 10/10\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 0.4781 - val_loss: 0.4664\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.46419\n"
     ]
    }
   ],
   "source": [
    "mc = ModelCheckpoint('models/best_model_note.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "history = model_note.fit(x=x_tr_note, y=y_tr_note,\n",
    "                    batch_size=128, epochs=10, \n",
    "                    validation_data=(x_val_note, y_val_note),\n",
    "                    verbose=1, callbacks=[mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 2)]           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               67072     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "=================================================================\n",
      "Total params: 100,096\n",
      "Trainable params: 100,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((n_timesteps, 2))\n",
    "x = LSTM(128)(inputs)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "output = Dense(128, activation=\"softmax\")(x)\n",
    "\n",
    "model_dur = tf.keras.Model(inputs, output)\n",
    "\n",
    "model_dur.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "model_dur.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "181/181 [==============================] - 5s 24ms/step - loss: 3.3678 - val_loss: 1.8395\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.83955, saving model to models/best_model_dur.h5\n",
      "Epoch 2/10\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6974 - val_loss: 1.6542\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.83955 to 1.65424, saving model to models/best_model_dur.h5\n",
      "Epoch 3/10\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6436 - val_loss: 1.6432\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.65424 to 1.64317, saving model to models/best_model_dur.h5\n",
      "Epoch 4/10\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 1.6364 - val_loss: 1.6394\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.64317 to 1.63941, saving model to models/best_model_dur.h5\n",
      "Epoch 5/10\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 1.6337 - val_loss: 1.6360\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.63941 to 1.63602, saving model to models/best_model_dur.h5\n",
      "Epoch 6/10\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 1.6320 - val_loss: 1.6350\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.63602 to 1.63499, saving model to models/best_model_dur.h5\n",
      "Epoch 7/10\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 1.6307 - val_loss: 1.6330\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.63499 to 1.63300, saving model to models/best_model_dur.h5\n",
      "Epoch 8/10\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 1.6301 - val_loss: 1.6344\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.63300\n",
      "Epoch 9/10\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 1.6296 - val_loss: 1.6326\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.63300 to 1.63261, saving model to models/best_model_dur.h5\n",
      "Epoch 10/10\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 1.6294 - val_loss: 1.6335\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.63261\n"
     ]
    }
   ],
   "source": [
    "mc = ModelCheckpoint('models/best_model_dur.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "history = model_dur.fit(x=x_tr_dur, y=y_tr_dur,\n",
    "                    batch_size=128, epochs=10, \n",
    "                    validation_data=(x_val_dur, y_val_dur),\n",
    "                    verbose=1, callbacks=[mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_note_sequence = x_val_note[np.random.randint(0, len(x_val_note))]\n",
    "# initial_duration_sequence = x_val_dur[np.random.randint(0, len(x_val_dur))]\n",
    "initial_sequence = x_val_note[np.random.randint(0, len(x_val_note))]\n",
    "x_int_to_note = {v: k for k, v in x_note_to_int.items()}\n",
    "x_int_to_dur = {v: k for k, v in x_dur_to_int.items()}\n",
    "\n",
    "produce_song(initial_sequence, x_int_to_note, x_int_to_dur, \n",
    "            n_notes=20, midi_file_path=\"songs/song.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
